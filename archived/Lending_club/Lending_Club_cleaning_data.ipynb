{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b> Purpose of this analysis</b></h1>\n",
    "Purpose of this mini project is to experiment some of data wrangling techniques. We have rcieved Lending Club Loan Data set from 2007 to 2015. There are total 2.2 million rows and 145 columns. It is important to do some pre-processing work in order to analyze data and fill out missing values.  \n",
    "\n",
    "<h2> <b> Packages</b></h2>\n",
    "We will start by importing some of packages. Following packages will be imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 1 : Reading Data in Dataframe </h2>\n",
    "Now we will be reading data from \"Loan.csv\" to Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->Read Chunk... 1\n",
      "-->Read Chunk... 2\n",
      "-->Read Chunk... 3\n",
      "-->Read Chunk... 4\n",
      "-->Read Chunk... 5\n",
      "-->Read Chunk... 6\n",
      "-->Read Chunk... 7\n",
      "-->Read Chunk... 8\n",
      "-->Read Chunk... 9\n",
      "-->Read Chunk... 10\n",
      "-->Read Chunk... 11\n",
      "-->Read Chunk... 12\n"
     ]
    }
   ],
   "source": [
    "file1 = \"loan.csv\"\n",
    "ChunkSize = 200000\n",
    "i = 1\n",
    "for chunk in pd.read_csv(file1, chunksize=ChunkSize, low_memory=False):\n",
    "    loan_data = chunk if i == 1 else pd.concat([loan_data, chunk])\n",
    "    print('-->Read Chunk...', i)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# data = pd.read_csv(\"loan.csv\", low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 2 : Checking data size </h2>\n",
    "Let's quickly check the shape of data.This will give us idea as how large is current dataset. As we can see, current data set contains more than 2 million rows and 145 columns. Not all columns are useful for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260668, 145)\n"
     ]
    }
   ],
   "source": [
    "# print(list(data.columns))\n",
    "print(loan_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 3 : Calculating Percentage of missing data per column </h2>\n",
    "Here  we will be creating Dataframe called \"df_null\". This Data frame consist of column name with percent of missing data in each column in descending order. It is noted that any column that has more than 60% of data missing are useless for our analysis and we will simply drop it. \n",
    "As we see some column like \"url\", \"id\", \"member_id\" has 100% missing values. Apart from that lot of columns have more than 60% missing data. We will simply drop column which has more than 60% data missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Count  Percent\n",
      "id                                          2260668    100.0\n",
      "url                                         2260668    100.0\n",
      "member_id                                   2260668    100.0\n",
      "orig_projected_additional_accrued_interest  2252242     99.6\n",
      "hardship_dpd                                2250055     99.5\n",
      "hardship_length                             2250055     99.5\n",
      "hardship_reason                             2250055     99.5\n",
      "hardship_status                             2250055     99.5\n",
      "deferral_term                               2250055     99.5\n",
      "hardship_amount                             2250055     99.5\n",
      "hardship_start_date                         2250055     99.5\n",
      "hardship_end_date                           2250055     99.5\n",
      "payment_plan_start_date                     2250055     99.5\n",
      "hardship_loan_status                        2250055     99.5\n",
      "hardship_type                               2250055     99.5\n",
      "hardship_payoff_balance_amount              2250055     99.5\n",
      "hardship_last_payment_amount                2250055     99.5\n",
      "settlement_amount                           2227612     98.5\n",
      "debt_settlement_flag_date                   2227612     98.5\n",
      "settlement_status                           2227612     98.5\n",
      "settlement_date                             2227612     98.5\n",
      "settlement_percentage                       2227612     98.5\n",
      "settlement_term                             2227612     98.5\n",
      "sec_app_mths_since_last_major_derog         2224726     98.4\n",
      "sec_app_revol_util                          2154484     95.3\n",
      "revol_bal_joint                             2152648     95.2\n",
      "sec_app_open_act_il                         2152647     95.2\n",
      "sec_app_num_rev_accts                       2152647     95.2\n",
      "sec_app_open_acc                            2152647     95.2\n",
      "sec_app_mort_acc                            2152647     95.2\n",
      "sec_app_inq_last_6mths                      2152647     95.2\n",
      "sec_app_earliest_cr_line                    2152647     95.2\n",
      "sec_app_chargeoff_within_12_mths            2152647     95.2\n",
      "sec_app_collections_12_mths_ex_med          2152647     95.2\n",
      "verification_status_joint                   2144938     94.9\n",
      "dti_joint                                   2139962     94.7\n",
      "annual_inc_joint                            2139958     94.7\n",
      "desc                                        2134601     94.4\n",
      "mths_since_last_record                      1901512     84.1\n",
      "mths_since_recent_bc_dlq                    1740967     77.0\n",
      "mths_since_last_major_derog                 1679893     74.3\n",
      "mths_since_recent_revol_delinq              1520309     67.3\n"
     ]
    }
   ],
   "source": [
    "# print((data.isna().sum()[data.isna().sum() > 0]))\n",
    "df_null = pd.DataFrame({'Count': loan_data.isnull().sum(), 'Percent': 100*loan_data.isnull().sum()/len(loan_data)})\n",
    "print(df_null[df_null['Percent'] >= 60].sort_values(by='Percent', ascending=False).round(1))\n",
    "\n",
    "\n",
    "\n",
    "# print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 4 : Dropping column with more than 60% missing data </h2>\n",
    "We will create a list of columns that a data missing more than 60%. We will simply drop those columns and create new dataframe called df_clean. We wil print shape of new dataframe to see how much data has been reduced. We can see total count of columns from 145 to reduced to 103. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "(2260668, 103)\n"
     ]
    }
   ],
   "source": [
    "# Creating list of column with more than 60% data missing. \n",
    "list_60 = list(df_null[df_null['Percent']>60].index)\n",
    "\n",
    "print(len(list_60))\n",
    "df_clean = loan_data.drop(list_60, axis=1)\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 5 : Analyzing columns with missing data  more than 10% and 60% </h2>\n",
    "Now that we have eliminated major columns that had missing values. Lets analyze remaining column with missing values. For that we will print all column that has missing data and see how we can predict missing values. We will predict all columns that has missing data grether than 10%. Since we already dropped columns that has missing data more than 60%, in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count    Percent\n",
      "mths_since_last_delinq  1158502  51.246003\n",
      "next_pymnt_d            1303607  57.664681\n",
      "open_acc_6m              866130  38.313012\n",
      "open_act_il              866129  38.312968\n",
      "open_il_12m              866129  38.312968\n",
      "open_il_24m              866129  38.312968\n",
      "mths_since_rcnt_il       909924  40.250227\n",
      "total_bal_il             866129  38.312968\n",
      "il_util                 1068850  47.280273\n",
      "open_rv_12m              866129  38.312968\n",
      "open_rv_24m              866129  38.312968\n",
      "max_bal_bc               866129  38.312968\n",
      "all_util                 866348  38.322655\n",
      "inq_fi                   866129  38.312968\n",
      "total_cu_tl              866130  38.313012\n",
      "inq_last_12m             866130  38.313012\n",
      "mths_since_recent_inq    295435  13.068482\n"
     ]
    }
   ],
   "source": [
    "df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "print(df_null[df_null['Percent'] > 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 5A : Checking data in columns with missing data more than 10% </h2>\n",
    "Lets examine few values of this columns so we can get idea what kind of data estimation we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mths_since_last_delinq next_pymnt_d  open_acc_6m  open_act_il  open_il_12m  \\\n",
      "0                     NaN     Mar-2019          2.0          2.0          1.0   \n",
      "1                    71.0     Mar-2019          4.0          4.0          2.0   \n",
      "2                     NaN     Mar-2019          0.0          1.0          0.0   \n",
      "\n",
      "   open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  \\\n",
      "0          2.0                 2.0       12560.0     69.0          2.0   \n",
      "1          3.0                 3.0       87153.0     88.0          4.0   \n",
      "2          2.0                14.0        7150.0     72.0          0.0   \n",
      "\n",
      "   open_rv_24m  max_bal_bc  all_util  inq_fi  total_cu_tl  inq_last_12m  \\\n",
      "0          7.0      2137.0      28.0     1.0         11.0           2.0   \n",
      "1          5.0       998.0      57.0     2.0         15.0           2.0   \n",
      "2          2.0         0.0      35.0     1.0          5.0           0.0   \n",
      "\n",
      "   mths_since_recent_inq  \n",
      "0                    2.0  \n",
      "1                    4.0  \n",
      "2                   14.0  \n"
     ]
    }
   ],
   "source": [
    "col_10 = list(df_null[df_null['Percent'] > 10].index)\n",
    "print(df_clean[col_10].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 5B : Dropping payment dates columns </h2>\n",
    "As we see above data set most missing data is numeric and we can use mode or median in order to fill missing values. By looking at dataset and name of columns, mode will be better option compared to median for most of columns. For \"total_bil_il\" we will use median. Lets analyze mode and median of those columns. But before we need to drop 3 columns which is not useful for our analysis. Those are \n",
    "1)  \"next_pymnt_dt\n",
    "2) 'last_credit_pull_d'\n",
    "3) 'last_pymnt_d'\n",
    "\n",
    "Lets create a list of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_clean.shape)\n",
    "cols_to_drop = ['next_pymnt_d', 'last_credit_pull_d', 'last_pymnt_d']\n",
    "df_clean.drop(cols_to_drop, axis=1, inplace=True)\n",
    "df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 5C : Checking mode and median of missing data columns </h2>\n",
    "Now we will eamine mode and median of all columns those we trying to do estiamtion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count  Percent  mode   median     mean\n",
      "mths_since_last_delinq  1158502     51.2  12.0     31.0     34.5\n",
      "open_acc_6m              866130     38.3   0.0      1.0      0.9\n",
      "open_act_il              866129     38.3   1.0      2.0      2.8\n",
      "open_il_12m              866129     38.3   0.0      0.0      0.7\n",
      "open_il_24m              866129     38.3   1.0      1.0      1.6\n",
      "mths_since_rcnt_il       909924     40.3   7.0     13.0     21.2\n",
      "total_bal_il             866129     38.3   0.0  23127.0  35506.6\n",
      "il_util                 1068850     47.3  78.0     72.0     69.1\n",
      "open_rv_12m              866129     38.3   0.0      1.0      1.3\n",
      "open_rv_24m              866129     38.3   1.0      2.0      2.7\n",
      "max_bal_bc               866129     38.3   0.0   4413.0   5806.4\n",
      "all_util                 866348     38.3  59.0     58.0     57.0\n",
      "inq_fi                   866129     38.3   0.0      1.0      1.0\n",
      "total_cu_tl              866130     38.3   0.0      0.0      1.5\n",
      "inq_last_12m             866130     38.3   0.0      1.0      2.0\n",
      "mths_since_recent_inq    295435     13.1   1.0      5.0      7.0\n"
     ]
    }
   ],
   "source": [
    "col_10 = list(df_null[df_null['Percent'] > 10].index)\n",
    "# print(col_10)\n",
    "# print(df_null.loc[col_10, :])\n",
    "for index in col_10:\n",
    "    df_null.loc[index, 'mode'] = df_clean[index].mode()[0]\n",
    "    df_null.loc[index, 'median'] = df_clean[index].median()\n",
    "    df_null.loc[index, 'mean'] = df_clean[index].mean()\n",
    "print(df_null.loc[col_10, :].round(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 5D : predicting missing values using mode </h2>\n",
    "By looking at above table, it makes more sense to fill all above columns by mode rather than median. Lets fill those missing values in above columns using mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[col_10] = df_clean[col_10].fillna(df_clean.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 6 : Dropping all missing data </h2>\n",
    "Now lets examine remaining missing data. Most columns will have missing data less than 10%. We will simply drop them. We may see lot of columns having missing data less than 10%. We will simply drop them. That way we will only lose 10% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1854073, 100)\n"
     ]
    }
   ],
   "source": [
    "df_clean.dropna(inplace=True)\n",
    "print(df_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get year info from Issue Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2018\n",
       "1    2018\n",
       "2    2018\n",
       "3    2018\n",
       "4    2018\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['year'] = pd.to_datetime(df_clean['issue_d']).dt.year\n",
    "df_clean = df_clean.drop('issue_d', axis=1)\n",
    "df_clean['year'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['percent_complete'] = (df_clean['total_pymnt']*100 / df_clean['loan_amnt']).round(2)\n",
    "# print(df_clean[df_clean['loan_status'] == \"Fully Paid\"]['percent_complete'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_clean['open_acc_6m'] = df_clean['open_acc_6m'].apply(lambda x: 3 if x>=3 else x)\n",
    "df_clean['open_act_il'] = df_clean['open_act_il'].apply(lambda x: 5 if x>=5 else x)\n",
    "df_clean['open_il_12m'] = df_clean['open_il_12m'].apply(lambda x: 3 if x>=3 else x)\n",
    "df_clean['open_rv_12m'] = df_clean['open_rv_12m'].apply(lambda x: 3 if x>=3 else x)\n",
    "df_clean['inq_fi'] = df_clean['inq_fi'].apply(lambda x: 3 if x>=3 else x)\n",
    "df_clean['inq_last_12m'] = df_clean['inq_last_12m'].apply(lambda x: 4 if x>=4 else x)\n",
    "df_clean['acc_open_past_24mths'] = df_clean['acc_open_past_24mths'].apply(lambda x: 8 if x>=8 else x)\n",
    "df_clean['mort_acc'] = df_clean['mort_acc'].apply(lambda x: 4 if x>=4 else x)\n",
    "df_clean['num_accts_ever_120_pd'] = df_clean['num_accts_ever_120_pd'].apply(lambda x: 3 if x>=3 else x)\n",
    "df_clean['num_tl_op_past_12m'] = df_clean['num_tl_op_past_12m'].apply(lambda x: 5 if x>=5 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (-0.001, 20.0]\n",
       "1    (-0.001, 20.0]\n",
       "2    (-0.001, 20.0]\n",
       "3     (80.0, 100.0]\n",
       "4    (-0.001, 20.0]\n",
       "Name: percent_bc_gt_75_cat, dtype: category\n",
       "Categories (5, interval[float64]): [(-0.001, 20.0] < (20.0, 40.0] < (40.0, 60.0] < (60.0, 80.0] < (80.0, 100.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['max_bal_bc_cat'] = pd.cut(x=df_clean['max_bal_bc'], bins=[0, 1000, 2000, 5000, 10000 ,1000000],\\\n",
    "                                    include_lowest=True)\n",
    "df_clean['num_sats_cat'] = pd.cut(x=df_clean['num_sats'], bins=[0, 5, 10, 20, 30 ,150],include_lowest=True)\n",
    "df_clean['pct_tl_nvr_dlq_cat'] = pd.cut(x=df_clean['pct_tl_nvr_dlq'], bins=[0, 60, 70, 80, 90 ,100],include_lowest=True)\n",
    "df_clean['percent_bc_gt_75_cat'] = pd.cut(x=df_clean['percent_bc_gt_75'], bins=[0, 20, 40, 60, 80 ,100],include_lowest=True)\n",
    "df_clean['percent_bc_gt_75_cat'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Out redundant values\n",
    "We will use value_count(normalize=True) function in order to find out redundant values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N    99.96\n",
      "Y     0.04\n",
      "Name: hardship_flag, dtype: float64\n",
      "---- mo_sin_old_il_acct ---\n",
      "Max value = 0.010000\n",
      "Length = 545\n",
      "---- mort_acc ---\n",
      "Max value = 0.410000\n",
      "Length = 5\n",
      "---- num_accts_ever_120_pd ---\n",
      "Max value = 0.760000\n",
      "Length = 4\n",
      "---- num_rev_accts ---\n",
      "Max value = 0.060000\n",
      "Length = 115\n",
      "---- num_rev_tl_bal_gt_0 ---\n",
      "Max value = 0.150000\n",
      "Length = 49\n",
      "---- num_sats_cat ---\n",
      "Max value = 0.450000\n",
      "Length = 5\n",
      "---- num_tl_120dpd_2m ---\n",
      "Max value = 1.000000\n",
      "Length = 7\n",
      "---- num_tl_30dpd ---\n",
      "Max value = 1.000000\n",
      "Length = 5\n",
      "---- num_tl_90g_dpd_24m ---\n",
      "Max value = 0.950000\n",
      "Length = 33\n",
      "---- num_tl_op_past_12m ---\n",
      "Max value = 0.260000\n",
      "Length = 6\n",
      "---- pct_tl_nvr_dlq_cat ---\n",
      "Max value = 0.780000\n",
      "Length = 5\n",
      "---- percent_bc_gt_75_cat ---\n",
      "Max value = 0.360000\n",
      "Length = 5\n",
      "---- pub_rec_bankruptcies ---\n",
      "Max value = 0.880000\n",
      "Length = 12\n",
      "---- tax_liens ---\n",
      "Max value = 0.970000\n",
      "Length = 40\n",
      "---- tot_hi_cred_lim ---\n",
      "Max value = 0.000000\n",
      "Length = 508102\n",
      "---- hardship_flag ---\n",
      "Max value = 1.000000\n",
      "Length = 2\n"
     ]
    }
   ],
   "source": [
    "col_list = ['mo_sin_old_il_acct', 'mort_acc', 'num_accts_ever_120_pd', 'num_rev_accts', 'num_rev_tl_bal_gt_0',\\\n",
    "            'num_sats_cat', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', \\\n",
    "           'pct_tl_nvr_dlq_cat', 'percent_bc_gt_75_cat', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', \\\n",
    "           'hardship_flag']\n",
    "# print(col_list)\n",
    "print(df_clean['hardship_flag'].value_counts(normalize=True).round(4)*100)\n",
    "# print(df_clean['num_sats'].nlargest(5))\n",
    "for c in col_list:\n",
    "    print (\"---- %s ---\" % c)\n",
    "    df_t = df_clean[c].value_counts(normalize=True).round(2)\n",
    "    \n",
    "    print(\"Max value = %f\" %df_t.max())\n",
    "    print(\"Length = %d\" %len(df_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "col_list2 = ['pymnt_plan', 'inq_last_6mths', 'mths_since_last_delinq', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util',\\\n",
    "            'total_acc', 'out_prncp', 'out_prncp_inv',  'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', \\\n",
    "             'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt',\\\n",
    "            'collections_12_mths_ex_med', 'policy_code',  'application_type',  'acc_now_delinq', 'tot_coll_amt', \\\n",
    "            'tot_cur_bal', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'open_rv_24m', 'max_bal_bc', \\\n",
    "             'all_util', 'total_rev_hi_lim', 'total_cu_tl', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \\\n",
    "             'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op',\\\n",
    "             'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mths_since_recent_bc', 'mths_since_recent_inq', 'num_actv_bc_tl',\\\n",
    "            'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', \\\n",
    "            'num_rev_tl_bal_gt_0', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'pct_tl_nvr_dlq_cat',\\\n",
    "            'pct_tl_nvr_dlq', 'pub_rec_bankruptcies', 'tax_liens', 'total_il_high_credit_limit', 'hardship_flag',\\\n",
    "            'disbursement_method', 'debt_settlement_flag', 'max_bal_bc', 'num_sats', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\\\n",
    "            'funded_amnt_inv', 'zip_code', 'funded_amnt',  'title', 'earliest_cr_line', ]\n",
    "print(len(col_list2))\n",
    "# df_clean['pymnt_plan'].value_counts(normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 7 : Analyzing datatypes of clean data </h2>\n",
    "Now we have get rid of all missing data. Dataset is almost clean. Lets see datatypes of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt                   int64\n",
      "term                       object\n",
      "int_rate                  float64\n",
      "installment               float64\n",
      "grade                      object\n",
      "sub_grade                  object\n",
      "emp_title                  object\n",
      "emp_length                 object\n",
      "home_ownership             object\n",
      "annual_inc                float64\n",
      "verification_status        object\n",
      "loan_status                object\n",
      "purpose                    object\n",
      "addr_state                 object\n",
      "dti                       float64\n",
      "delinq_2yrs               float64\n",
      "initial_list_status        object\n",
      "open_acc_6m               float64\n",
      "open_act_il               float64\n",
      "open_il_12m               float64\n",
      "il_util                   float64\n",
      "open_rv_12m               float64\n",
      "inq_fi                    float64\n",
      "inq_last_12m              float64\n",
      "acc_open_past_24mths      float64\n",
      "mort_acc                  float64\n",
      "num_accts_ever_120_pd     float64\n",
      "num_tl_op_past_12m        float64\n",
      "tot_hi_cred_lim           float64\n",
      "total_bal_ex_mort         float64\n",
      "total_bc_limit            float64\n",
      "year                        int64\n",
      "percent_complete          float64\n",
      "max_bal_bc_cat           category\n",
      "num_sats_cat             category\n",
      "percent_bc_gt_75_cat     category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(df_clean.dtypes)\n",
    "\n",
    "df_clean2 = df_clean.drop(col_list2, axis=1) \n",
    "\n",
    "print(df_clean2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 8 : Fixing \"Employment Length\" column </h2>\n",
    "In above datatypes one of the important title is employment length. We should be expecting it as integer or float but it is object type. Lets analyze this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10+ years    665309\n",
      "2 years      179041\n",
      "3 years      158836\n",
      "< 1 year     157037\n",
      "1 year       129053\n",
      "5 years      121646\n",
      "4 years      119208\n",
      "6 years       89893\n",
      "7 years       81825\n",
      "8 years       81520\n",
      "9 years       70705\n",
      "Name: emp_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean2['emp_length'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets remove unnecessary keywords suh as \"year\", \"<\" and \"+\". We will consider all records of empoyment length <1 years as 0 years and 10+ years as 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    665309\n",
      "2     179041\n",
      "3     158836\n",
      "0     157037\n",
      "1     129053\n",
      "5     121646\n",
      "4     119208\n",
      "6      89893\n",
      "7      81825\n",
      "8      81520\n",
      "9      70705\n",
      "Name: emp_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df_clean['emp_length'].astype('str').dtypes\n",
    "df_clean2['emp_length'] = df_clean2['emp_length'].str.replace(r'[+]\\s|[a-z]', '')\n",
    "df_clean2['emp_length'] = df_clean2['emp_length'].str.replace('< 1', '0')\n",
    "# df_clean['emp_length'] = df_clean['emp_length'].str.replace('10+ ', '10')\n",
    "\n",
    "print(df_clean2['emp_length'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Loan and Bad Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean2['year'] = pd.to_datetime(df_clean2['issue_d']).dt.year\n",
    "# print(train_data['loan_status'].value_counts())\n",
    "# train_data['status_var'] = train_data['loan_status'].map({'Fully Paid':0, 'Charged Off':1, 'Late (31-120 days)':1, \\\n",
    "#                                                         'In Grace Period':1, 'Late (16-30 days)':1, 'Default':1})\n",
    "# print(train_data['status_var'].value_counts())\n",
    "loan_status_dict = {\"Fully Paid\": 0, \"Charged Off\": 1, \"Late (31-120 days)\": 1,\n",
    "                    \"In Grace Period\": 1, \"Late (16-30 days)\": 1, \"Default\": 1, \"Current\" : 2}\n",
    "# known_status = df_clean2['loan_status'] != 'Current'\n",
    "df_clean2[\"loan_status_count\"] = df_clean2[\"loan_status\"].map(loan_status_dict)\n",
    "df_clean2[\"loan_status_count\"] = df_clean2[\"loan_status_count\"].astype(\"int\")\n",
    "\n",
    "# loan_known_data = df_clean2[known_status].reset_index()\n",
    "loan_status_dict = {0.0: \"Good loans\", 1.0: \"Bad loans\", 2.0: 'Unknown'}\n",
    "df_clean2[\"loan_status\"] = df_clean2[\"loan_status_count\"].map(loan_status_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 9 : Analyzing clean data </h2>\n",
    "Now that we have clean data, we will analyze its shape. Majority of columns are numeric and hence we will use dataframe.describe function to just see summary of numeric columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1854073, 37)\n",
      "   loan_amnt        term  int_rate  installment grade sub_grade  \\\n",
      "0       2500   36 months     13.56        84.92     C        C1   \n",
      "1      30000   60 months     18.94       777.23     D        D2   \n",
      "2       5000   36 months     17.97       180.69     D        D1   \n",
      "3       4000   36 months     18.94       146.51     D        D2   \n",
      "4      30000   60 months     16.14       731.78     C        C4   \n",
      "\n",
      "        emp_title emp_length home_ownership  annual_inc verification_status  \\\n",
      "0            Chef         10           RENT    55000.00        Not Verified   \n",
      "1     Postmaster          10       MORTGAGE    90000.00     Source Verified   \n",
      "2  Administrative         6        MORTGAGE    59280.00     Source Verified   \n",
      "3   IT Supervisor         10       MORTGAGE    92000.00     Source Verified   \n",
      "4        Mechanic         10       MORTGAGE    57250.00        Not Verified   \n",
      "\n",
      "  loan_status             purpose addr_state   dti  delinq_2yrs  \\\n",
      "0     Unknown  debt_consolidation         NY 18.24         0.00   \n",
      "1     Unknown  debt_consolidation         LA 26.52         0.00   \n",
      "2     Unknown  debt_consolidation         MI 10.51         0.00   \n",
      "3     Unknown  debt_consolidation         WA 16.74         0.00   \n",
      "4     Unknown  debt_consolidation         MD 26.35         0.00   \n",
      "\n",
      "  initial_list_status  open_acc_6m  open_act_il  open_il_12m  il_util  \\\n",
      "0                   w         2.00         2.00         1.00    69.00   \n",
      "1                   w         3.00         4.00         2.00    88.00   \n",
      "2                   w         0.00         1.00         0.00    72.00   \n",
      "3                   w         1.00         5.00         3.00    68.00   \n",
      "4                   w         3.00         5.00         3.00    89.00   \n",
      "\n",
      "   open_rv_12m  inq_fi  inq_last_12m  acc_open_past_24mths  mort_acc  \\\n",
      "0         2.00    1.00          2.00                  8.00      0.00   \n",
      "1         3.00    2.00          2.00                  8.00      3.00   \n",
      "2         0.00    1.00          0.00                  4.00      2.00   \n",
      "3         0.00    2.00          3.00                  5.00      3.00   \n",
      "4         2.00    1.00          0.00                  8.00      2.00   \n",
      "\n",
      "   num_accts_ever_120_pd  num_tl_op_past_12m  tot_hi_cred_lim  \\\n",
      "0                   0.00                3.00         60124.00   \n",
      "1                   0.00                5.00        372872.00   \n",
      "2                   0.00                0.00        136927.00   \n",
      "3                   0.00                3.00        385183.00   \n",
      "4                   0.00                5.00        157548.00   \n",
      "\n",
      "   total_bal_ex_mort  total_bc_limit  year  percent_complete  \\\n",
      "0           16901.00        36500.00  2018              6.68   \n",
      "1           99468.00        15000.00  2018              5.02   \n",
      "2           11749.00        13800.00  2018              7.08   \n",
      "3           36151.00         5000.00  2018              7.17   \n",
      "4           29674.00         9300.00  2018              4.74   \n",
      "\n",
      "     max_bal_bc_cat  num_sats_cat percent_bc_gt_75_cat  loan_status_count  \n",
      "0  (2000.0, 5000.0]   (5.0, 10.0]       (-0.001, 20.0]                  2  \n",
      "1  (-0.001, 1000.0]  (10.0, 20.0]       (-0.001, 20.0]                  2  \n",
      "2  (-0.001, 1000.0]   (5.0, 10.0]       (-0.001, 20.0]                  2  \n",
      "3  (2000.0, 5000.0]   (5.0, 10.0]        (80.0, 100.0]                  2  \n",
      "4  (-0.001, 1000.0]  (10.0, 20.0]       (-0.001, 20.0]                  2  \n"
     ]
    }
   ],
   "source": [
    "# df_clean3 = df_clean2.drop('loan_status')\n",
    "print(df_clean2.shape)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(df_clean2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 10: Exporting clean data to 'csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2.to_csv(\"Loan_cleandata2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
