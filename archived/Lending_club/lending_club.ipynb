{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <b> Purpose of this analysis</b></h1>\n",
    "Purpose of this mini project is to experiment some of data wrangling techniques. We have rcieved Lending Club Loan Data set from 2007 to 2015. There are total 2.2 million rows and 145 columns. It is important to do some pre-processing work in order to analyze data and fill out missing values.  \n",
    "\n",
    "<h1> <b> Packages</b></h1>\n",
    "We will start by importing some of packages. Following packages will be imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be reading data from \"Loan.csv\" to Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"loan.csv\", low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the shape of data.This will give us idea as how large is current dataset. As we can see, current data set contains more than 2 million rows and 145 columns. Not all columns are useful for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260668, 145)\n"
     ]
    }
   ],
   "source": [
    "# print(list(data.columns))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here  we will be creating Dataframe called \"df_null\". This Data frame consist of column name with percent of missing data in each column in descending order. It is noted that any column that has more than 60% of data missing are useless for our analysis and we will simply drop it. \n",
    "As we see some column like \"url\", \"id\", \"member_id\" has 100% missing values. Apart from that lot of columns have more than 80% missing data. We will simply drop column which has more than 60% data missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Count     Percent\n",
      "id                                          2260668  100.000000\n",
      "url                                         2260668  100.000000\n",
      "member_id                                   2260668  100.000000\n",
      "orig_projected_additional_accrued_interest  2252242   99.627278\n",
      "hardship_length                             2250055   99.530537\n",
      "hardship_reason                             2250055   99.530537\n",
      "hardship_status                             2250055   99.530537\n",
      "deferral_term                               2250055   99.530537\n",
      "hardship_amount                             2250055   99.530537\n",
      "hardship_start_date                         2250055   99.530537\n",
      "hardship_end_date                           2250055   99.530537\n",
      "payment_plan_start_date                     2250055   99.530537\n",
      "hardship_loan_status                        2250055   99.530537\n",
      "hardship_dpd                                2250055   99.530537\n",
      "hardship_payoff_balance_amount              2250055   99.530537\n",
      "hardship_last_payment_amount                2250055   99.530537\n",
      "hardship_type                               2250055   99.530537\n",
      "debt_settlement_flag_date                   2227612   98.537777\n",
      "settlement_status                           2227612   98.537777\n",
      "settlement_date                             2227612   98.537777\n",
      "settlement_amount                           2227612   98.537777\n",
      "settlement_percentage                       2227612   98.537777\n",
      "settlement_term                             2227612   98.537777\n",
      "sec_app_mths_since_last_major_derog         2224726   98.410116\n",
      "sec_app_revol_util                          2154484   95.302981\n",
      "revol_bal_joint                             2152648   95.221766\n",
      "sec_app_open_acc                            2152647   95.221722\n",
      "sec_app_chargeoff_within_12_mths            2152647   95.221722\n",
      "sec_app_open_act_il                         2152647   95.221722\n",
      "sec_app_collections_12_mths_ex_med          2152647   95.221722\n",
      "sec_app_mort_acc                            2152647   95.221722\n",
      "sec_app_inq_last_6mths                      2152647   95.221722\n",
      "sec_app_earliest_cr_line                    2152647   95.221722\n",
      "sec_app_num_rev_accts                       2152647   95.221722\n",
      "verification_status_joint                   2144938   94.880717\n",
      "dti_joint                                   2139962   94.660605\n",
      "annual_inc_joint                            2139958   94.660428\n",
      "desc                                        2134601   94.423462\n",
      "mths_since_last_record                      1901512   84.112837\n",
      "mths_since_recent_bc_dlq                    1740967   77.011175\n",
      "mths_since_last_major_derog                 1679893   74.309585\n",
      "mths_since_recent_revol_delinq              1520309   67.250432\n",
      "next_pymnt_d                                1303607   57.664681\n",
      "mths_since_last_delinq                      1158502   51.246003\n",
      "il_util                                     1068850   47.280273\n",
      "mths_since_rcnt_il                           909924   40.250227\n",
      "all_util                                     866348   38.322655\n",
      "inq_last_12m                                 866130   38.313012\n",
      "total_cu_tl                                  866130   38.313012\n",
      "open_acc_6m                                  866130   38.313012\n",
      "open_il_24m                                  866129   38.312968\n",
      "open_act_il                                  866129   38.312968\n",
      "inq_fi                                       866129   38.312968\n",
      "max_bal_bc                                   866129   38.312968\n",
      "open_rv_24m                                  866129   38.312968\n",
      "open_rv_12m                                  866129   38.312968\n",
      "total_bal_il                                 866129   38.312968\n",
      "open_il_12m                                  866129   38.312968\n",
      "mths_since_recent_inq                        295435   13.068482\n",
      "emp_title                                    166969    7.385826\n",
      "num_tl_120dpd_2m                             153657    6.796973\n",
      "emp_length                                   146907    6.498389\n",
      "mo_sin_old_il_acct                           139071    6.151766\n",
      "bc_util                                       76071    3.364979\n",
      "percent_bc_gt_75                              75379    3.334368\n",
      "bc_open_to_buy                                74935    3.314728\n",
      "mths_since_recent_bc                          73412    3.247359\n",
      "pct_tl_nvr_dlq                                70431    3.115495\n",
      "avg_cur_bal                                   70346    3.111735\n",
      "num_rev_accts                                 70277    3.108683\n",
      "mo_sin_rcnt_rev_tl_op                         70277    3.108683\n",
      "mo_sin_old_rev_tl_op                          70277    3.108683\n",
      "num_tl_90g_dpd_24m                            70276    3.108639\n",
      "num_actv_rev_tl                               70276    3.108639\n",
      "tot_coll_amt                                  70276    3.108639\n",
      "tot_cur_bal                                   70276    3.108639\n",
      "total_rev_hi_lim                              70276    3.108639\n",
      "num_tl_op_past_12m                            70276    3.108639\n",
      "num_op_rev_tl                                 70276    3.108639\n",
      "num_il_tl                                     70276    3.108639\n",
      "mo_sin_rcnt_tl                                70276    3.108639\n",
      "num_accts_ever_120_pd                         70276    3.108639\n",
      "num_actv_bc_tl                                70276    3.108639\n",
      "num_rev_tl_bal_gt_0                           70276    3.108639\n",
      "num_tl_30dpd                                  70276    3.108639\n",
      "tot_hi_cred_lim                               70276    3.108639\n",
      "num_bc_tl                                     70276    3.108639\n",
      "total_il_high_credit_limit                    70276    3.108639\n",
      "num_bc_sats                                   58590    2.591712\n",
      "num_sats                                      58590    2.591712\n",
      "acc_open_past_24mths                          50030    2.213063\n",
      "mort_acc                                      50030    2.213063\n",
      "total_bc_limit                                50030    2.213063\n",
      "total_bal_ex_mort                             50030    2.213063\n",
      "title                                         23325    1.031775\n",
      "last_pymnt_d                                   2426    0.107313\n",
      "revol_util                                     1802    0.079711\n",
      "dti                                            1711    0.075686\n",
      "pub_rec_bankruptcies                           1365    0.060380\n",
      "chargeoff_within_12_mths                        145    0.006414\n",
      "collections_12_mths_ex_med                      145    0.006414\n",
      "tax_liens                                       105    0.004645\n",
      "last_credit_pull_d                               73    0.003229\n",
      "inq_last_6mths                                   30    0.001327\n",
      "open_acc                                         29    0.001283\n",
      "total_acc                                        29    0.001283\n",
      "earliest_cr_line                                 29    0.001283\n",
      "delinq_2yrs                                      29    0.001283\n",
      "delinq_amnt                                      29    0.001283\n",
      "acc_now_delinq                                   29    0.001283\n",
      "pub_rec                                          29    0.001283\n",
      "annual_inc                                        4    0.000177\n",
      "zip_code                                          1    0.000044\n"
     ]
    }
   ],
   "source": [
    "# print((data.isna().sum()[data.isna().sum() > 0]))\n",
    "df_null = pd.DataFrame({'Count': data.isnull().sum(), 'Percent': 100*data.isnull().sum()/len(data)})\n",
    "print(df_null[df_null['Percent'] > 0].sort_values(by='Percent', ascending=False))\n",
    "\n",
    "\n",
    "# print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list of columns that a data missing more than 60%. We will simply drop those columns and create new dataframe called df_clean. We wil print shape of new dataframe to see how much data has been reduced. We can see total count of columns from 145 to reduced to 103. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260668, 103)\n"
     ]
    }
   ],
   "source": [
    "# Creating list of column with more than 60% data missing. \n",
    "list_60 = list(df_null[df_null['Percent']>60].index)\n",
    "\n",
    "df_clean = data.drop(list_60, axis=1)\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have eliminated major columns that had missing values. Lets analyze remaining column with missing values. For that we will print all column that has missing data and see how we can predict missing values. We will predict all columns that has missing data grether than 10%. Since we already dropped columns that has missing data more than 60%, in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count    Percent\n",
      "mths_since_last_delinq  1158502  51.246003\n",
      "next_pymnt_d            1303607  57.664681\n",
      "open_acc_6m              866130  38.313012\n",
      "open_act_il              866129  38.312968\n",
      "open_il_12m              866129  38.312968\n",
      "open_il_24m              866129  38.312968\n",
      "mths_since_rcnt_il       909924  40.250227\n",
      "total_bal_il             866129  38.312968\n",
      "il_util                 1068850  47.280273\n",
      "open_rv_12m              866129  38.312968\n",
      "open_rv_24m              866129  38.312968\n",
      "max_bal_bc               866129  38.312968\n",
      "all_util                 866348  38.322655\n",
      "inq_fi                   866129  38.312968\n",
      "total_cu_tl              866130  38.313012\n",
      "inq_last_12m             866130  38.313012\n",
      "mths_since_recent_inq    295435  13.068482\n"
     ]
    }
   ],
   "source": [
    "# print(df_clean.isna().sum())\n",
    "# print(df_clean.info())\n",
    "\n",
    "df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "print(df_null[df_null['Percent'] > 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine few values of this columns so we can get idea what kind of data estimation we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mths_since_last_delinq next_pymnt_d  open_acc_6m  open_act_il  open_il_12m  \\\n",
      "0                     NaN     Mar-2019          2.0          2.0          1.0   \n",
      "1                    71.0     Mar-2019          4.0          4.0          2.0   \n",
      "2                     NaN     Mar-2019          0.0          1.0          0.0   \n",
      "\n",
      "   open_il_24m  mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  \\\n",
      "0          2.0                 2.0       12560.0     69.0          2.0   \n",
      "1          3.0                 3.0       87153.0     88.0          4.0   \n",
      "2          2.0                14.0        7150.0     72.0          0.0   \n",
      "\n",
      "   open_rv_24m  max_bal_bc  all_util  inq_fi  total_cu_tl  inq_last_12m  \\\n",
      "0          7.0      2137.0      28.0     1.0         11.0           2.0   \n",
      "1          5.0       998.0      57.0     2.0         15.0           2.0   \n",
      "2          2.0         0.0      35.0     1.0          5.0           0.0   \n",
      "\n",
      "   mths_since_recent_inq  \n",
      "0                    2.0  \n",
      "1                    4.0  \n",
      "2                   14.0  \n"
     ]
    }
   ],
   "source": [
    "col_10 = list(df_null[df_null['Percent'] > 10].index)\n",
    "print(df_clean[col_10].head(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above data set most missing data is numeric and we can use mode or median in order to fill missing values. By looking at dataset and name of columns, mode will be better option compared to median for most of columns. For \"total_bil_il\" we will use median. Lets analyze mode and median of those columns. But before we need to drop 3 columns which is not useful for our analysis. Those are \n",
    "1)  \"next_pymnt_dt\n",
    "2) 'last_credit_pull_d'\n",
    "3) 'last_pymnt_d'\n",
    "\n",
    "Lets create a list of these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_clean.shape)\n",
    "cols_to_drop = ['next_pymnt_d', 'last_credit_pull_d', 'last_pymnt_d']\n",
    "df_clean.drop(cols_to_drop, axis=1, inplace=True)\n",
    "df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will eamine mode and median of all columns those we trying to do estiamtion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count    Percent  mode   median\n",
      "mths_since_last_delinq  1158502  51.246003  12.0     31.0\n",
      "open_acc_6m              866130  38.313012   0.0      1.0\n",
      "open_act_il              866129  38.312968   1.0      2.0\n",
      "open_il_12m              866129  38.312968   0.0      0.0\n",
      "open_il_24m              866129  38.312968   1.0      1.0\n",
      "mths_since_rcnt_il       909924  40.250227   7.0     13.0\n",
      "total_bal_il             866129  38.312968   0.0  23127.0\n",
      "il_util                 1068850  47.280273  78.0     72.0\n",
      "open_rv_12m              866129  38.312968   0.0      1.0\n",
      "open_rv_24m              866129  38.312968   1.0      2.0\n",
      "max_bal_bc               866129  38.312968   0.0   4413.0\n",
      "all_util                 866348  38.322655  59.0     58.0\n",
      "inq_fi                   866129  38.312968   0.0      1.0\n",
      "total_cu_tl              866130  38.313012   0.0      0.0\n",
      "inq_last_12m             866130  38.313012   0.0      1.0\n",
      "mths_since_recent_inq    295435  13.068482   1.0      5.0\n"
     ]
    }
   ],
   "source": [
    "# # df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "\n",
    "# # ##printing columns with null count more than 0\n",
    "# # # print(df_null)\n",
    "# df_clean['annual_inc'].fillna(df_clean['annual_inc'].median(), inplace=True)\n",
    "# df_clean['emp_length'].fillna(df_clean['emp_length'].median(), inplace=True)\n",
    "\n",
    "# df_clean['emp_title'].fillna('Unknown', inplace=True)\n",
    "# df_clean['title'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# cols_0 = ['open_il_12m', 'open_il_24m', 'total_bal_il', 'open_rv_12m', 'open_rv_24m', 'total_cu_tl', \\\n",
    "#         'inq_last_12m', 'inq_fi', 'max_bal_bc', 'total_il_high_credit_limit', 'total_bc_limit', 'total_bal_ex_mort', \\\n",
    "#          'percent_bc_gt_75', 'num_tl_op_past_12m', 'num_tl_90g_dpd_24m', 'num_tl_30dpd', 'num_tl_120dpd_2m', \\\n",
    "#           'num_accts_ever_120_pd', 'mort_acc', 'tot_coll_amt', 'tot_cur_bal', 'avg_cur_bal', 'bc_open_to_buy',\\\n",
    "#           'bc_util', 'inq_last_6mths', 'collections_12_mths_ex_med', 'chargeoff_within_12_mths', 'pub_rec_bankruptcies',\\\n",
    "#          'tax_liens']\n",
    "# df_clean[cols_0] = df_clean[cols_0].fillna(0)\n",
    "\n",
    "# cols_mode = ['mths_since_rcnt_il', 'il_util', 'all_util', 'total_rev_hi_lim', 'acc_open_past_24mths',\\\n",
    "#             'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\\\n",
    "#             'mths_since_recent_bc', 'mths_since_recent_inq', 'num_actv_bc_tl', 'num_actv_rev_tl',\\\n",
    "#             'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\\\n",
    "#             'num_rev_tl_bal_gt_0', 'num_sats', 'pct_tl_nvr_dlq', 'tot_hi_cred_lim', 'delinq_2yrs', 'dti', 'revol_util',\\\n",
    "#            'open_acc_6m', 'open_act_il', 'mths_since_last_delinq']\n",
    "# df_clean[cols_mode] = df_clean[cols_mode].fillna(df_clean.mode().iloc[0])\n",
    "\n",
    "# # print(df_clean['title'].dtypes)\n",
    "# df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "# index_list = list(df_null.index)\n",
    "col_10 = list(df_null[df_null['Percent'] > 10].index)\n",
    "# print(col_10)\n",
    "# print(df_null.loc[col_10, :])\n",
    "for index in col_10:\n",
    "    df_null.loc[index, 'mode'] = df_clean[index].mode()[0]\n",
    "    df_null.loc[index, 'median'] = df_clean[index].median()\n",
    "print(df_null.loc[col_10, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at above table, it makes more sense to fill all above columns by mode rather than median. Lets fill those missing values in above columns using mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[col_10] = df_clean[col_10].fillna(df_clean.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets examine remaining missing data. Most columns will have missing data less than 10%. We will simply drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean['annual_inc'].fillna(df_clean['annual_inc'].median(), inplace=True)\n",
    "# df_clean['emp_length'].fillna(df_clean['emp_length'].median(), inplace=True)\n",
    "\n",
    "# df_clean['emp_title'].fillna('Unknown', inplace=True)\n",
    "# df_clean['title'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# cols_0 = ['open_il_12m', 'open_il_24m', 'total_bal_il', 'open_rv_12m', 'open_rv_24m', 'total_cu_tl', \\\n",
    "#         'inq_last_12m', 'inq_fi', 'max_bal_bc', 'total_il_high_credit_limit', 'total_bc_limit', 'total_bal_ex_mort', \\\n",
    "#          'percent_bc_gt_75', 'num_tl_op_past_12m', 'num_tl_90g_dpd_24m', 'num_tl_30dpd', 'num_tl_120dpd_2m', \\\n",
    "#           'num_accts_ever_120_pd', 'mort_acc', 'tot_coll_amt', 'tot_cur_bal', 'avg_cur_bal', 'bc_open_to_buy',\\\n",
    "#           'bc_util', 'inq_last_6mths', 'collections_12_mths_ex_med', 'chargeoff_within_12_mths', 'pub_rec_bankruptcies',\\\n",
    "#          'tax_liens']\n",
    "# df_clean[cols_0] = df_clean[cols_0].fillna(0)\n",
    "\n",
    "# cols_mode = ['mths_since_rcnt_il', 'il_util', 'all_util', 'total_rev_hi_lim', 'acc_open_past_24mths',\\\n",
    "#             'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl',\\\n",
    "#             'mths_since_recent_bc', 'mths_since_recent_inq', 'num_actv_bc_tl', 'num_actv_rev_tl',\\\n",
    "#             'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\\\n",
    "#             'num_rev_tl_bal_gt_0', 'num_sats', 'pct_tl_nvr_dlq', 'tot_hi_cred_lim', 'delinq_2yrs', 'dti', 'revol_util',\\\n",
    "#            'open_acc_6m', 'open_act_il', 'mths_since_last_delinq']\n",
    "# df_clean[cols_mode] = df_clean[cols_mode].fillna(df_clean.mode().iloc[0])\n",
    "\n",
    "# # print(df_clean['title'].dtypes)\n",
    "# df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "# index_list = list(df_null.index)\n",
    "\n",
    "# for index in index_list:\n",
    "#     df_null.loc[index, 'mode'] = df_clean[index].mode()[0]\n",
    "# print([df_null[df_null['Percent']>0.2].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = ['next_pymnt_d', 'last_credit_pull_d', 'last_pymnt_d']\n",
    "# df_clean.drop(drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # print(df_clean['earliest_cr_line'].mode())\n",
    "# print(df_clean['inq_last_6mths'].median())\n",
    "# print(df_clean['emp_length'].value_counts())\n",
    "# print(df_clean['emp_length'].astype('str').dtypes)\n",
    "# df_clean['emp_length'] = df_clean['emp_length'].str.replace(r'[+]\\s|[a-z]', '')\n",
    "# df_clean['emp_length'] = df_clean['emp_length'].str.replace('< 1', '0')\n",
    "# # df_clean['emp_length'] = df_clean['emp_length'].str.replace('10+ ', '10')\n",
    "\n",
    "# print(df_clean['emp_length'].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = df_clean[pd.notnull(df_clean['acc_now_delinq'])]\n",
    "# df_null = pd.DataFrame({'Count': df_clean.isnull().sum(), 'Percent': 100*df_clean.isnull().sum()/len(df_clean)})\n",
    "# index_list = list(df_null.index)\n",
    "# for index in index_list:\n",
    "#     df_null.loc[index, 'mode'] = df_clean[index].mode()[0]\n",
    "\n",
    "\n",
    "# print(df_null[df_null['Percent']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_int = ['emp_length', ]\n",
    "# df_clean[col_int] = df_clean[col_int].astype('int')\n",
    "# print(df_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['X'].describe().apply(\"{0:.5f}\".format)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y = 'int_rate', x = 'grade', data=df_clean, order = sorted(df_clean.grade.unique()) , col='term', kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(df_clean.grade.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot( x = 'home_ownership', data = df_clean, kind = 'count', hue='term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean['home_ownership'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loan, ax_loan = plt.subplots(2, 2, figsize=(15, 10))\n",
    "sns.distplot(loan_data[\"loan_amnt\"], ax=ax_loan[0, 0], color=\"#F7522F\")\n",
    "sns.violinplot(y=loan_data[\"loan_amnt\"], ax=ax_loan[0, 1], inner=\"quartile\", palette=\"Reds\")\n",
    "sns.distplot(loan_data[\"funded_amnt\"], ax=ax_loan[1, 0], color=\"#2F8FF7\")\n",
    "sns.violinplot(y=loan_data[\"funded_amnt\"], ax=ax_loan[1, 1], inner=\"quartile\", palette=\"Blues\")\n",
    "ax_loan[0, 0].set_title(\"Loan amount distribution\", fontsize=16)\n",
    "ax_loan[0, 1].set_title(\"Loan amount distribution\", fontsize=16)\n",
    "ax_loan[1, 0].set_title(\"Funded amount distribution\", fontsize=16)\n",
    "ax_loan[1, 1].set_title(\"Funded amount distribution\", fontsize=16)\n",
    "ax_loan[0, 0].set_xlabel(\"loan amount\", fontsize=16)\n",
    "ax_loan[1, 0].set_xlabel(\"loan amount\", fontsize=16)\n",
    "ax_loan[0, 1].set_ylabel(\"loan amount\", fontsize=16)\n",
    "ax_loan[1, 1].set_ylabel(\"loan amount\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['year'] = pd.to_datetime(loan_data['issue_d']).dt.year\n",
    "loan_year_num = loan_data['year'].value_counts().to_dict()\n",
    "loan_year_num_pd = pd.DataFrame(list(loan_year_num.items()), columns=[\"year\", \"loan times\"])\n",
    "loan_year_num_pd.sort_values(\"year\", inplace=True)\n",
    "# print(loan_year_num_pd)\n",
    "loan_money_count_per_year = loan_data.groupby(\"year\")[\"loan_amnt\"].sum().to_dict()\n",
    "loan_money_count_per_year_pd = pd.DataFrame(list(loan_money_count_per_year.items()), columns=[\"year\", \"loan_amnt\"])\n",
    "loan_money_count_per_year_pd.sort_values(\"year\", inplace=True)\n",
    "# print(loan_money_count_per_year_pd)\n",
    "\n",
    "# fig, ax = plt.subplots(1,2, figsize = (15,10))\n",
    "sns.catplot(x='year', y = 'loan times', data = loan_year_num_pd, palette = 'tab10', kind='bar')\n",
    "sns.catplot(x='year', y = 'loan_amnt', data = loan_money_count_per_year_pd, palette = 'tab10', kind='bar' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loan_amnt_per_state = loan_data[['addr_state', 'loan_amnt']].groupby('addr_state').sum().sort_values(by='loan_amnt', ascending=False).reset_index()\n",
    "# print(loan_amnt_per_state)\n",
    "loan_per_state = loan_data['addr_state'].value_counts().rename_axis('addr_state').reset_index(name='counts')\n",
    "# print(loan_per_state)\n",
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(15,30))\n",
    "sns.barplot(x='loan_amnt', y = 'addr_state', data = loan_amnt_per_state, orient = 'h', ax=ax1 )\n",
    "sns.barplot(x='counts', y = 'addr_state', data = loan_per_state, orient = 'h', ax=ax2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_amnt_per_job = loan_data[['emp_title', 'loan_amnt']].groupby('emp_title').mean().sort_values(by = 'loan_amnt', ascending = False).reset_index()\n",
    "# loan_per_job = loan_data['emp_title'].value_counts().rename_axis('emp_title').reset_index(name = 'counts')\n",
    "# fig, (ax1, ax2) = plt.subplots(2,1,figsize=(15,30))\n",
    "# sns.barplot(x='loan_amnt', y = 'emp_title', data = loan_amnt_per_job, orient = 'h', ax=ax1 )\n",
    "# sns.barplot(x='counts', y = 'emp_title', data = loan_per_job, orient = 'h', ax=ax2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
